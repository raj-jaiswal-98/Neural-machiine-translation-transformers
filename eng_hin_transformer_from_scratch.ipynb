{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64504682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:11.082513Z",
     "iopub.status.busy": "2023-05-12T14:23:11.082083Z",
     "iopub.status.idle": "2023-05-12T14:23:23.207374Z",
     "shell.execute_reply": "2023-05-12T14:23:23.205609Z"
    },
    "papermill": {
     "duration": 12.141885,
     "end_time": "2023-05-12T14:23:23.209562",
     "exception": false,
     "start_time": "2023-05-12T14:23:11.067677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "pip install -q bpemb  # byte pair encoding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a63bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:23.237118Z",
     "iopub.status.busy": "2023-05-12T14:23:23.235671Z",
     "iopub.status.idle": "2023-05-12T14:23:34.590514Z",
     "shell.execute_reply": "2023-05-12T14:23:34.589413Z"
    },
    "papermill": {
     "duration": 11.370489,
     "end_time": "2023-05-12T14:23:34.592642",
     "exception": false,
     "start_time": "2023-05-12T14:23:23.222153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9b99f3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:34.619430Z",
     "iopub.status.busy": "2023-05-12T14:23:34.619086Z",
     "iopub.status.idle": "2023-05-12T14:23:39.144606Z",
     "shell.execute_reply": "2023-05-12T14:23:39.143656Z"
    },
    "papermill": {
     "duration": 4.541226,
     "end_time": "2023-05-12T14:23:39.146677",
     "exception": false,
     "start_time": "2023-05-12T14:23:34.605451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from bpemb import BPEmb\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e190833d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:39.175070Z",
     "iopub.status.busy": "2023-05-12T14:23:39.173198Z",
     "iopub.status.idle": "2023-05-12T14:23:48.562218Z",
     "shell.execute_reply": "2023-05-12T14:23:48.561317Z"
    },
    "papermill": {
     "duration": 9.405111,
     "end_time": "2023-05-12T14:23:48.564576",
     "exception": false,
     "start_time": "2023-05-12T14:23:39.159465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               hindi  \\\n",
       "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "\n",
       "                                          english  \n",
       "0  Give your application an accessibility workout  \n",
       "1               Accerciser Accessibility Explorer  \n",
       "2  The default plugin layout for the bottom panel  \n",
       "3     The default plugin layout for the top panel  \n",
       "4  A list of plugins that are disabled by default  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('hindi_english_parallel.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8e4335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:48.592429Z",
     "iopub.status.busy": "2023-05-12T14:23:48.592115Z",
     "iopub.status.idle": "2023-05-12T14:23:52.268997Z",
     "shell.execute_reply": "2023-05-12T14:23:52.267525Z"
    },
    "papermill": {
     "duration": 3.693208,
     "end_time": "2023-05-12T14:23:52.271055",
     "exception": false,
     "start_time": "2023-05-12T14:23:48.577847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1353912 entries, 0 to 1561839\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count    Dtype \n",
      "---  ------   --------------    ----- \n",
      " 0   hindi    1353912 non-null  object\n",
      " 1   english  1353912 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae6a6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:52.299371Z",
     "iopub.status.busy": "2023-05-12T14:23:52.299045Z",
     "iopub.status.idle": "2023-05-12T14:23:56.814503Z",
     "shell.execute_reply": "2023-05-12T14:23:56.811215Z"
    },
    "papermill": {
     "duration": 4.532944,
     "end_time": "2023-05-12T14:23:56.817842",
     "exception": false,
     "start_time": "2023-05-12T14:23:52.284898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "      <th>english_len</th>\n",
       "      <th>hindi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               hindi  \\\n",
       "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "\n",
       "                                          english  english_len  hindi_len  \n",
       "0  Give your application an accessibility workout            6          8  \n",
       "1               Accerciser Accessibility Explorer            3          3  \n",
       "2  The default plugin layout for the bottom panel            8          7  \n",
       "3     The default plugin layout for the top panel            8          7  \n",
       "4  A list of plugins that are disabled by default            9         12  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['english_len'] = data['english'].apply(lambda x:len(x.split()))\n",
    "data['hindi_len'] = data['hindi'].apply(lambda x:len(x.split()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533ecd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:56.865785Z",
     "iopub.status.busy": "2023-05-12T14:23:56.865345Z",
     "iopub.status.idle": "2023-05-12T14:23:56.953116Z",
     "shell.execute_reply": "2023-05-12T14:23:56.952205Z"
    },
    "papermill": {
     "duration": 0.118226,
     "end_time": "2023-05-12T14:23:56.957377",
     "exception": false,
     "start_time": "2023-05-12T14:23:56.839151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "      <th>english_len</th>\n",
       "      <th>hindi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्स...</td>\n",
       "      <td>The duration of the highlight box when selecti...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               hindi  \\\n",
       "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "6  पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्स...   \n",
       "\n",
       "                                             english  english_len  hindi_len  \n",
       "0     Give your application an accessibility workout            6          8  \n",
       "2     The default plugin layout for the bottom panel            8          7  \n",
       "3        The default plugin layout for the top panel            8          7  \n",
       "4     A list of plugins that are disabled by default            9         12  \n",
       "6  The duration of the highlight box when selecti...           10         10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[(data.english_len>=5) & (data.english_len<=15) & (data.hindi_len>=5) & (data.hindi_len<=15)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6279ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:57.001860Z",
     "iopub.status.busy": "2023-05-12T14:23:57.001476Z",
     "iopub.status.idle": "2023-05-12T14:23:57.042235Z",
     "shell.execute_reply": "2023-05-12T14:23:57.041277Z"
    },
    "papermill": {
     "duration": 0.065714,
     "end_time": "2023-05-12T14:23:57.045343",
     "exception": false,
     "start_time": "2023-05-12T14:23:56.979629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.sample(n=25000, random_state=0)\n",
    "train_split, test_split = train_test_split(data, test_size=0.1, random_state=0)\n",
    "train_split = train_split.reset_index(0).drop(['index'], axis=1)\n",
    "test_split = test_split.reset_index(0).drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d5246a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:57.086467Z",
     "iopub.status.busy": "2023-05-12T14:23:57.086095Z",
     "iopub.status.idle": "2023-05-12T14:23:57.103691Z",
     "shell.execute_reply": "2023-05-12T14:23:57.102874Z"
    },
    "papermill": {
     "duration": 0.040655,
     "end_time": "2023-05-12T14:23:57.106538",
     "exception": false,
     "start_time": "2023-05-12T14:23:57.065883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500 2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "      <th>english_len</th>\n",
       "      <th>hindi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>एक खूब छोटा अंश जिस में तत्वों के गुण होते है।</td>\n",
       "      <td>A very small component acquiring a quality of ...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>कोटा जानकारी समर्थित नहीं फ़ोल्डर '% s' के लिए</td>\n",
       "      <td>No IMAP mailbox available for folder '% s'</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>परन्तु यह शीघ्र ही अपर्याप्त प्रतीत हुआ...।</td>\n",
       "      <td>But soon this was found rather inadequate.</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>उसने गिरफ्तार व्यक्ति के लिए प्रतिभू की भूमिका...</td>\n",
       "      <td>He acted as ad - promisor for the arrested per...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>अल्लाह से क्षमा की प्रार्थना करो। निस्संदेह अल...</td>\n",
       "      <td>Ask God for forgiveness: He is most forgiving ...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               hindi  \\\n",
       "0    एक खूब छोटा अंश जिस में तत्वों के गुण होते है।    \n",
       "1     कोटा जानकारी समर्थित नहीं फ़ोल्डर '% s' के लिए   \n",
       "2       परन्तु यह शीघ्र ही अपर्याप्त प्रतीत हुआ...।    \n",
       "3  उसने गिरफ्तार व्यक्ति के लिए प्रतिभू की भूमिका...   \n",
       "4  अल्लाह से क्षमा की प्रार्थना करो। निस्संदेह अल...   \n",
       "\n",
       "                                             english  english_len  hindi_len  \n",
       "0  A very small component acquiring a quality of ...            9         11  \n",
       "1         No IMAP mailbox available for folder '% s'            8          9  \n",
       "2         But soon this was found rather inadequate.            7          7  \n",
       "3  He acted as ad - promisor for the arrested per...           10          9  \n",
       "4  Ask God for forgiveness: He is most forgiving ...           10         12  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_split), len(test_split))\n",
    "train_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79dd48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T06:59:14.777540Z",
     "iopub.status.busy": "2023-05-11T06:59:14.777082Z",
     "iopub.status.idle": "2023-05-11T06:59:14.788484Z",
     "shell.execute_reply": "2023-05-11T06:59:14.786870Z",
     "shell.execute_reply.started": "2023-05-11T06:59:14.777505Z"
    },
    "papermill": {
     "duration": 0.019331,
     "end_time": "2023-05-12T14:23:57.149006",
     "exception": false,
     "start_time": "2023-05-12T14:23:57.129675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization approach\n",
    "One of the ways to perform subword tokenization is Byte-Pair Encoding (BPE, actually it is a data compression algorithm), \n",
    "WordPiece (used by BERT), and SentencePiece \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d9312d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:23:57.188806Z",
     "iopub.status.busy": "2023-05-12T14:23:57.188438Z",
     "iopub.status.idle": "2023-05-12T14:24:04.526844Z",
     "shell.execute_reply": "2023-05-12T14:24:04.525898Z"
    },
    "papermill": {
     "duration": 7.361858,
     "end_time": "2023-05-12T14:24:04.530249",
     "exception": false,
     "start_time": "2023-05-12T14:23:57.168391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is basically the module by which we're gonna perform tokenization according to the Byte-Level Byte Pair Encoding\n",
    "bpemb_en = BPEmb(lang='en')\n",
    "bpemb_hi = BPEmb(lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17173c",
   "metadata": {
    "papermill": {
     "duration": 0.016084,
     "end_time": "2023-05-12T14:24:04.571458",
     "exception": false,
     "start_time": "2023-05-12T14:24:04.555374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0072afec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:04.605283Z",
     "iopub.status.busy": "2023-05-12T14:24:04.604927Z",
     "iopub.status.idle": "2023-05-12T14:24:04.615673Z",
     "shell.execute_reply": "2023-05-12T14:24:04.615025Z"
    },
    "papermill": {
     "duration": 0.029937,
     "end_time": "2023-05-12T14:24:04.617488",
     "exception": false,
     "start_time": "2023-05-12T14:24:04.587551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, max_seq_len=64):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        eng_sen = self.data.english.iloc[index]\n",
    "        hin_sen = self.data.hindi.iloc[index]\n",
    "        eng_tokens = bpemb_en.encode_ids_with_bos_eos(eng_sen)\n",
    "        hin_tokens = bpemb_hi.encode_ids_with_bos_eos(hin_sen)\n",
    "        trg_input_tokens = hin_tokens[:-1]\n",
    "        trg_output_tokens = hin_tokens[1:]\n",
    "        \n",
    "        eng_mask = [1]*(len(eng_tokens))\n",
    "        hin_mask = [1]*(len(trg_input_tokens))\n",
    "        \n",
    "        eng_tokens = eng_tokens + [0]*(self.max_seq_len - len(eng_tokens))\n",
    "        trg_input_tokens = trg_input_tokens + [0]*(self.max_seq_len - len(trg_input_tokens))\n",
    "        trg_output_tokens = trg_output_tokens + [0]*(self.max_seq_len - len(trg_output_tokens))\n",
    "        \n",
    "        eng_mask = eng_mask + [0]*(self.max_seq_len - len(eng_mask))\n",
    "        hin_mask = hin_mask + [0]*(self.max_seq_len - len(hin_mask))\n",
    "        # pad eng_tokens upto max_seq_len\n",
    "        # pad_hin_tokens upto max_seq_len\n",
    "        # then create masks for both of the inputs, and make them upto the dimension needed\n",
    "        \n",
    "        # now we have to pad the sequence upto max length and create the masks for the same\n",
    "        \n",
    "        \n",
    "        return torch.tensor(eng_tokens), torch.tensor(trg_input_tokens), torch.tensor(trg_output_tokens), torch.tensor(eng_mask), torch.tensor(hin_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7be77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T06:19:32.852927Z",
     "iopub.status.busy": "2023-05-09T06:19:32.852547Z",
     "iopub.status.idle": "2023-05-09T06:19:32.862981Z",
     "shell.execute_reply": "2023-05-09T06:19:32.861836Z",
     "shell.execute_reply.started": "2023-05-09T06:19:32.852900Z"
    },
    "papermill": {
     "duration": 0.016132,
     "end_time": "2023-05-12T14:24:04.649919",
     "exception": false,
     "start_time": "2023-05-12T14:24:04.633787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoder\n",
    "So the solution is 'throw all recurrence and base EACH encoder OUTPUT on all the ENCODER INPUTS', so the solution is \n",
    "self-attention.\n",
    "For example let there be n inputs, vocab_size be 10, and embedding_dim be m, so our first we have to get the attention weights\n",
    "which will be like - softmax([<xj,x1>, <xj, x2>,...,  <xj,xn>]) and the new resultant vector is x_i = sum(a_ij*x_j) which will\n",
    "be of shape n again.\n",
    "Now due to this type of mechanism we can avoid recurrence, and get all the attention based outputs in just a single pass. \n",
    "By just multiplying the matrices as softmax(X@X.T, axis=0) (if input is of shape n ,embedding_dim where n i sthe number of\n",
    "time_steps or seq_len). (Note that this is for one senetence only). \n",
    "And we can stack attention as they are of the same shape. \n",
    "\n",
    "But the probelm in above task is that, we don't have any learnable weights, so we have to rely only on the quality of \n",
    "embeddingss, so we insert weights in the scenario and it becomes scaled dot product self-attention.\n",
    "\n",
    "\n",
    "Attention(Q, K, V) = softmax(Q@K.T/sqrt(d_k))V, where Q = X@W_q, K = X@W_k, V = X@W_v.  And each of W_q, W_k, W_v is of shape \n",
    "dxd and X is of shape (txd). This is known as attention single head. Scaling in this helps in preventing the exploding \n",
    "gradient problem. \n",
    "\n",
    "Now there is still a problem in the above implementation as the self attention is going to give the most weight to one of the \n",
    "embedding_dimension, but there can be multiple informations in a single statement, like 'I went to a restaurant to meet my \n",
    "freind that night', there are questions, 'why, where, who, when'. So. we require multiple such attentions so that our \n",
    "attention mechanism takes care of all such queries, so the solution is multi-headed attention, in this method we take\n",
    "separate weights for W_q, W_k, W_v for each of the single heads, but the catch over here is that each of matrix W_q, W_k, W_v\n",
    "is of the shape is (dxd/h), thus the computational cost is still same as single headed attention. And then, the outputs from \n",
    "all the heads are then passed concatenated and passed through a feed forward network. \n",
    "\n",
    "In all this mechanism, we got the information on how the words are dependent on one another, but we lost the positional \n",
    "information, so here comes the positional encoding for rescue. We will add the positional embedding layer (which can be treated\n",
    "as learnable weights) and it is of the same shape as our inputs.\n",
    "\n",
    "Now there is no non-linearity in the system, thus we have to introduce one, so we insert feedforward networks after mutli-head\n",
    "self attention with non-linear activation functions, now all this multihead + feedforward is reffered as an encoder block, and \n",
    "we can stack these blocks.\n",
    "\n",
    "But stacking brings another problem, i.e., the positional informations gets lost over if multiple encoder blocks are used and \n",
    "this also leads to vanishing gradient problem.\n",
    "\n",
    "\n",
    "And the solutions to both these problems are LayerNormalization in between feedforward networks and skip-connections between \n",
    "the blocks (same as resnet).\n",
    "\n",
    "\n",
    "# Decoder\n",
    "In decoder, we can apply the same process, but there is a issue, as in encoder, we have the all the input sentence at one time,\n",
    "but in decoding practically, we have only the previous time steps, so we have to somwhow zero the attention weights given to \n",
    "the subsequent words in the sentence. Thus we have to mask them. Thus, it is known as masked smulti-head self attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce2cb4",
   "metadata": {
    "papermill": {
     "duration": 0.015793,
     "end_time": "2023-05-12T14:24:04.681653",
     "exception": false,
     "start_time": "2023-05-12T14:24:04.665860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shapes of inputs and outputs of different layers\n",
    "m - number of examples or batch_size if batched inputs\n",
    "\n",
    "t - sequence length or can say number of time steps\\\n",
    "\n",
    "d - model shape, or embedding dimension shape \n",
    "\n",
    "h - number of heads\n",
    "\n",
    "Then, \n",
    "\n",
    "1. input shape - (m, t, d)\n",
    "2. W_q, W_k, W_v - (d, d/h)\n",
    "3. Q,K,V shapes - XW_q.shape i.e. (m, t, d/h)\n",
    "4. Shape of K.T will be (m, d/h, t)\n",
    "5. Output from single head - softmax(Q@K.T/sqrt(d_k)) where d_k = d/h, will be - (m, t, d/h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e8febe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:04.715445Z",
     "iopub.status.busy": "2023-05-12T14:24:04.714758Z",
     "iopub.status.idle": "2023-05-12T14:24:04.722182Z",
     "shell.execute_reply": "2023-05-12T14:24:04.721348Z"
    },
    "papermill": {
     "duration": 0.026553,
     "end_time": "2023-05-12T14:24:04.724180",
     "exception": false,
     "start_time": "2023-05-12T14:24:04.697627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    # this is going in the single head self-attention.\n",
    "    # query, key, value shape will be (b, h, t, d/h) \n",
    "    d_k = query.shape[-1]\n",
    "    scaled_scores = torch.matmul(query, torch.transpose(key, -2, -1))/np.sqrt(d_k)  # shape is  (b, h, t, t)\n",
    "    \n",
    "    if mask is not None:\n",
    "        # mask must be of shape (b,h,t,t)\n",
    "        scaled_scores = torch.where(mask==0, -np.inf, scaled_scores)\n",
    "        \n",
    "    weights = torch.nn.Softmax(dim=-1)(scaled_scores) # shape is (b,h,t,t)\n",
    "    return torch.matmul(weights, value)  # shape will be (b,h,t,d/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831687b1",
   "metadata": {
    "papermill": {
     "duration": 0.016336,
     "end_time": "2023-05-12T14:24:05.099951",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.083615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bfc3e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.134551Z",
     "iopub.status.busy": "2023-05-12T14:24:05.133806Z",
     "iopub.status.idle": "2023-05-12T14:24:05.144427Z",
     "shell.execute_reply": "2023-05-12T14:24:05.143592Z"
    },
    "papermill": {
     "duration": 0.029965,
     "end_time": "2023-05-12T14:24:05.146392",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.116427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # let us say that our input is of shape (b, t, d) \n",
    "        self.wq = nn.Linear(in_features = self.d_model, out_features = self.d_model, bias = False) \n",
    "        self.wk = nn.Linear(in_features = self.d_model, out_features = self.d_model, bias = False)\n",
    "        self.wv = nn.Linear(in_features = self.d_model, out_features = self.d_model, bias = False)\n",
    "        # remember that wq, wk, ev defined above are just the matrices nothing more\n",
    "        \n",
    "        # now the final dense layer to add some nolinearity in it \n",
    "        self.fc = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # shape of q, k, v is (b, t, d)\n",
    "        q = self.wq(q).reshape(q.shape[0], q.shape[1], self.num_heads, self.d_model//self.num_heads).permute(0, 2, 1, 3) \n",
    "        k = self.wk(k).reshape(k.shape[0], k.shape[1], self.num_heads, self.d_model//self.num_heads).permute(0, 2, 1, 3) \n",
    "        v = self.wv(v).reshape(v.shape[0], v.shape[1], self.num_heads, self.d_model//self.num_heads).permute(0, 2, 1, 3) \n",
    "        \n",
    "        # now shape of q, k, v is (b, h, t, d/h)\n",
    "        # now we have to simply perform  self-attention for every head\n",
    "        op = scaled_dot_product_attention(q, k, v, mask)  # shape of op is (b, h, t, d/h)\n",
    "        op = op.permute(0, 2, 1, 3)\n",
    "        op = op.reshape(op.shape[0], op.shape[1], self.d_model)  # now shape of op is (b, t, d)\n",
    "        return self.fc(op) # shapes are (b, t, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29395e9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.181053Z",
     "iopub.status.busy": "2023-05-12T14:24:05.180344Z",
     "iopub.status.idle": "2023-05-12T14:24:05.186288Z",
     "shell.execute_reply": "2023-05-12T14:24:05.185470Z"
    },
    "papermill": {
     "duration": 0.025247,
     "end_time": "2023-05-12T14:24:05.188268",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.163021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class feedforward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super(feedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752dad23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.222375Z",
     "iopub.status.busy": "2023-05-12T14:24:05.222076Z",
     "iopub.status.idle": "2023-05-12T14:24:05.229799Z",
     "shell.execute_reply": "2023-05-12T14:24:05.228886Z"
    },
    "papermill": {
     "duration": 0.027105,
     "end_time": "2023-05-12T14:24:05.231722",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.204617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super(encoder_block, self).__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.fc = feedforward(d_model, hidden_dim) # gonna give the shapes again to be (b, t, d)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # input shape is (b, t, d)\n",
    "        op = self.mhsa(x, x, x, mask) # op shape is (b, t, d) and attention weights are of shape (b, t, t)\n",
    "        op = self.dropout1(op)\n",
    "        # now we have to pass it through layer normalization (study it)\n",
    "        op = self.layernorm1(op + x)\n",
    "        ffn_op = self.fc(op)\n",
    "        ffn_op = self.dropout2(ffn_op)\n",
    "        op = self.layernorm2(op + ffn_op)\n",
    "        return op\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c1a091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.266956Z",
     "iopub.status.busy": "2023-05-12T14:24:05.266194Z",
     "iopub.status.idle": "2023-05-12T14:24:05.276004Z",
     "shell.execute_reply": "2023-05-12T14:24:05.275238Z"
    },
    "papermill": {
     "duration": 0.029407,
     "end_time": "2023-05-12T14:24:05.277928",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.248521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class encoder_transformer(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        # max_seq_len is the number of time steps, i'll be referring it as t\n",
    "        super(encoder_transformer, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.vocab_size = src_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.token_embeds = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_embeds = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.blocks = nn.ModuleList([encoder_block(d_model, num_heads, hidden_dim, dropout_rate=dropout_rate) \n",
    "                      for _ in range(self.num_blocks)])\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, source, mask=None):\n",
    "        # shape of source is (b, t)\n",
    "        # all source sentences will be padded and padding will be static\n",
    "        source = source.type(torch.LongTensor).to(device)\n",
    "        # comment out above line for \n",
    "        t_embeds = self.token_embeds(source) # (b, t, d)\n",
    "        pos_ids = torch.broadcast_to(torch.arange(self.max_seq_len), (x.shape[0], self.max_seq_len)).type(torch.LongTensor\n",
    "                                                                                                         ).to(device)\n",
    "        p_embeds = self.pos_embeds(pos_ids) # (b, t, d)\n",
    "        \n",
    "        inp = t_embeds + p_embeds  # (b, t, d)\n",
    "        op = self.dropout(inp)  # (b, t, d)\n",
    "        \n",
    "        for _, block in enumerate(self.blocks):\n",
    "            op = block(op, mask)\n",
    "            \n",
    "        return op\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f03a1",
   "metadata": {
    "papermill": {
     "duration": 0.016205,
     "end_time": "2023-05-12T14:24:05.310531",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.294326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da20a9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.345225Z",
     "iopub.status.busy": "2023-05-12T14:24:05.344483Z",
     "iopub.status.idle": "2023-05-12T14:24:05.353889Z",
     "shell.execute_reply": "2023-05-12T14:24:05.352977Z"
    },
    "papermill": {
     "duration": 0.028736,
     "end_time": "2023-05-12T14:24:05.355844",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.327108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super(decoder_block, self).__init__()\n",
    "        self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        \n",
    "        self.fc = feedforward(d_model, hidden_dim)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_ouptut, target, decoder_mask=None, memory_mask=None):\n",
    "        mhsa_op1 = self.mhsa1(target, target, target, decoder_mask)\n",
    "        mhsa_op1 = self.dropout1(mhsa_op1)\n",
    "        mhsa_op1 = self.layernorm1(mhsa_op1 + target)\n",
    "        \n",
    "        mhsa_op2 = self.mhsa2(mhsa_op1, encoder_ouptut, encoder_ouptut, memory_mask)\n",
    "        mhsa_op2 = self.dropout2(mhsa_op2)\n",
    "        mhsa_op2 = self.layernorm2(mhsa_op1 + mhsa_op2)\n",
    "        \n",
    "        fc_op = self.fc(mhsa_op2)\n",
    "        fc_op = self.dropout3(fc_op)\n",
    "        op = self.layernorm3(fc_op + mhsa_op2)\n",
    "        \n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "939f5a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.390480Z",
     "iopub.status.busy": "2023-05-12T14:24:05.389743Z",
     "iopub.status.idle": "2023-05-12T14:24:05.398861Z",
     "shell.execute_reply": "2023-05-12T14:24:05.398041Z"
    },
    "papermill": {
     "duration": 0.028394,
     "end_time": "2023-05-12T14:24:05.400955",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.372561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class decoder_transformer(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, trg_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(decoder_transformer, self).__init__()\n",
    "        self.token_embeds = nn.Embedding(trg_vocab_size, d_model)\n",
    "        self.pos_embeds = nn.Embedding(max_seq_len, d_model)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([decoder_block(d_model, num_heads, hidden_dim, dropout_rate) \n",
    "                                    for _ in range(num_blocks)])\n",
    "    \n",
    "    def forward(self, encoder_output, target, decoder_mask=None, memory_mask=None):\n",
    "        # shape of target is (b, t)\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        t_embeds = self.token_embeds(target) # (b, t, d)\n",
    "        pos_ids = torch.broadcast_to(torch.arange(self.max_seq_len), (x.shape[0], self.max_seq_len)).type(torch.LongTensor\n",
    "                                                                                                         ).to(device)\n",
    "        p_embeds = self.pos_embeds(pos_ids)  # (b, t, d)\n",
    "        \n",
    "        inp = t_embeds + p_embeds\n",
    "        op = self.dropout(inp)\n",
    "        # now op is (b, t, d)\n",
    "        \n",
    "        for _, block in enumerate(self.blocks):\n",
    "            op = block(encoder_output, op, decoder_mask, memory_mask)\n",
    "            \n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acfb229f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.435574Z",
     "iopub.status.busy": "2023-05-12T14:24:05.434776Z",
     "iopub.status.idle": "2023-05-12T14:24:05.444315Z",
     "shell.execute_reply": "2023-05-12T14:24:05.443519Z"
    },
    "papermill": {
     "duration": 0.028905,
     "end_time": "2023-05-12T14:24:05.446411",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.417506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size, trg_vocab_size, max_seq_len, \n",
    "                dropout_rate=0.1):\n",
    "        super(transformer, self).__init__()\n",
    "        self.encoder = encoder_transformer(num_blocks, d_model, num_heads, hidden_dim, src_vocab_size, max_seq_len,\n",
    "                            dropout_rate=0.1)\n",
    "        self.decoder = decoder_transformer(num_blocks, d_model, num_heads, hidden_dim, src_vocab_size, max_seq_len,\n",
    "                            dropout_rate=0.1)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, trg_vocab_size)\n",
    "        self.lookahead = torch.tril(torch.ones((max_seq_len, max_seq_len))).to(device)\n",
    "    \n",
    "    def forward(self, src, trg, src_pad_mask=None, trg_pad_mask=None):\n",
    "        # we require masks to be of shape (b, 1, 1, t)\n",
    "        dec_mask = None\n",
    "        if src_pad_mask is not None:\n",
    "            src_pad_mask = src_pad_mask.unsqueeze(1).unsqueeze(1)\n",
    "            src_pad_mask = src_pad_mask.to(device)\n",
    "        if trg_pad_mask is not None:\n",
    "            trg_pad_mask = trg_pad_mask.unsqueeze(1).unsqueeze(1)\n",
    "            dec_mask = torch.minimum(trg_pad_mask, self.lookahead)\n",
    "            dec_mask = dec_mask.to(device)\n",
    "            \n",
    "        enc_op = self.encoder(src, src_pad_mask)\n",
    "        op = self.decoder(enc_op, trg, dec_mask, src_pad_mask) # op is of shape (b, t, d)\n",
    "        # and generate the required look ahead mask\n",
    "        op = op.reshape(-1, op.shape[-1]) # shape is (b*t, d)\n",
    "        op = self.fc(op) # shape is (b*t, trg_vocab_size)\n",
    "        \n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e5346",
   "metadata": {
    "papermill": {
     "duration": 0.017213,
     "end_time": "2023-05-12T14:24:05.480523",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.463310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab7ece99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.515301Z",
     "iopub.status.busy": "2023-05-12T14:24:05.514557Z",
     "iopub.status.idle": "2023-05-12T14:24:05.518853Z",
     "shell.execute_reply": "2023-05-12T14:24:05.518033Z"
    },
    "papermill": {
     "duration": 0.023327,
     "end_time": "2023-05-12T14:24:05.520658",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.497331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BLEU():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b7c28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.555503Z",
     "iopub.status.busy": "2023-05-12T14:24:05.554779Z",
     "iopub.status.idle": "2023-05-12T14:24:05.558751Z",
     "shell.execute_reply": "2023-05-12T14:24:05.557853Z"
    },
    "papermill": {
     "duration": 0.023397,
     "end_time": "2023-05-12T14:24:05.560660",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.537263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def METEOR():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37561591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.595013Z",
     "iopub.status.busy": "2023-05-12T14:24:05.594240Z",
     "iopub.status.idle": "2023-05-12T14:24:05.598933Z",
     "shell.execute_reply": "2023-05-12T14:24:05.598146Z"
    },
    "papermill": {
     "duration": 0.023751,
     "end_time": "2023-05-12T14:24:05.600820",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.577069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='my_checkpoint(1).pth'):\n",
    "    # will save model and optimizer params at every epoch\n",
    "    print(\"-> Saving CheckPoint\")\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d5c0354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.635220Z",
     "iopub.status.busy": "2023-05-12T14:24:05.634479Z",
     "iopub.status.idle": "2023-05-12T14:24:05.639241Z",
     "shell.execute_reply": "2023-05-12T14:24:05.638435Z"
    },
    "papermill": {
     "duration": 0.023793,
     "end_time": "2023-05-12T14:24:05.641107",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.617314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model):\n",
    "    # it will just load, we can train it further, make changes to the architecture\n",
    "    # and simply use it to predict\n",
    "    print(\"-> Loading CheckPoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2896b52d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.675410Z",
     "iopub.status.busy": "2023-05-12T14:24:05.674607Z",
     "iopub.status.idle": "2023-05-12T14:24:05.684090Z",
     "shell.execute_reply": "2023-05-12T14:24:05.683307Z"
    },
    "papermill": {
     "duration": 0.028523,
     "end_time": "2023-05-12T14:24:05.685998",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.657475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(loader, model, optimizer, scaler, scheduler, loss_fn, epoch, device=device):\n",
    "    '''\n",
    "    it is the training procedure for one epoch of the network\n",
    "    '''\n",
    "    losses = 0\n",
    "    model.train()\n",
    "    num_batches = len(loader)\n",
    "    batches = tqdm(loader) # tqdm will be used to generate progress bars\n",
    "    for idx, batch in enumerate(batches, 0):\n",
    "        src = batch[0].to(device)  # (batch_size, max_len)\n",
    "        trg_inp = batch[1].to(device)  # (batch_size, max_len)\n",
    "        trg_op = batch[2].to(device) # (batch_size, max_len)\n",
    "        src_pad_mask = batch[3].to(device) # (batch_size, max_len)\n",
    "        trg_pad_mask = batch[4].to(device) # (batch_size, max_len)\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(): # for gradient underflowing and overflowing and it makes training faster by converting all floats to float16\n",
    "            op = model(src, trg_inp, src_pad_mask, trg_pad_mask) # op shape is (batch_size*max_len, trg_vocab_size+1)\n",
    "            trg_op = trg_op.reshape(trg_op.shape[0]*trg_op.shape[1]) # trg_op shape is (batch_size*max_len)\n",
    "            loss = loss_fn(op, trg_op) # loss_fn should contain the parametere ignore_idx=0, so that \n",
    "            # losses corresponding to the padding token isn't calculated\n",
    "\n",
    "        # making all the previous gradients zero \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        batches.set_postfix(loss = loss.item(), epoch=epoch) # loss of this current batch on current iteration \n",
    "        losses+= loss.item()\n",
    "\n",
    "    losses/=num_batches    \n",
    "    #scheduler.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cf1ce0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.720485Z",
     "iopub.status.busy": "2023-05-12T14:24:05.719781Z",
     "iopub.status.idle": "2023-05-12T14:24:05.724099Z",
     "shell.execute_reply": "2023-05-12T14:24:05.723324Z"
    },
    "papermill": {
     "duration": 0.023376,
     "end_time": "2023-05-12T14:24:05.725966",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.702590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # i will only use this function for measuring its accuracy on different metrics\n",
    "    # for this task such as meteor and bleu\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5c5c442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.760274Z",
     "iopub.status.busy": "2023-05-12T14:24:05.760008Z",
     "iopub.status.idle": "2023-05-12T14:24:05.769959Z",
     "shell.execute_reply": "2023-05-12T14:24:05.769181Z"
    },
    "papermill": {
     "duration": 0.02934,
     "end_time": "2023-05-12T14:24:05.771855",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.742515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, max_seq_len=64, trans_len=50, beam_search = None):\n",
    "    model.eval()\n",
    "    # first i am given the english sentence\n",
    "    inp = bpemb_en.encode_ids_with_bos_eos(sentence)\n",
    "    enc_mask = [1]*(len(inp))\n",
    "    inp = inp + [0]*(max_seq_len - len(inp))\n",
    "    enc_mask = enc_mask + [0]*(max_seq_len - len(enc_mask))\n",
    "    # inp shape is (max_seq_len) and so is of mask\n",
    "    #print(f'input = {inp}\\n\\nmask = {enc_mask}')\n",
    "    inp = torch.tensor(inp).unsqueeze(0).to(device) # shape of input is (1, max_seq_len)\n",
    "    enc_mask = torch.tensor(enc_mask).unsqueeze(0).to(device) # shape of mask is also (1, max_seq_len)\n",
    "    #inp =  inp.unsqueeze(0)\n",
    "    #enc_mask = enc_mask.unsqueeze(0).unsqueeze(1).unsqueeze(1) #  shape should be (b,1,1,max_seq_len)\n",
    "    # now the shapes are as required by the transformer\n",
    "    \n",
    "    #enc_op = model.encoder(inp, enc_mask)\n",
    "    # now we have to decode the sentence one-by-one \n",
    "    # so let us first of all make the inputs and the corresponding trg_mask\n",
    "    trg_inp = torch.zeros(max_seq_len).unsqueeze(0).to(device) # shape is (1, max_seq_len)\n",
    "    trg_mask = torch.zeros(max_seq_len).unsqueeze(0).to(device) # shape is (1, max_seq_len)\n",
    "    \n",
    "    #lookahead = torch.tril(torch.ones((max_seq_len, max_seq_len))).to(device) \n",
    "    \n",
    "    trg_inp[0, 0] = 1 # 1 means <sos> token\n",
    "    translation = []\n",
    "    #trg_mask[0, 0, 0, len(translation)] = 1 # as trg_inp has only one word in it at current time step\n",
    "    #dec_mask = torch.minimum(trg_mask, lookahead)\n",
    "    trg_mask[0, len(translation)] = 1 # as trg_inp has only one word in it at current time step\n",
    "    last_token = trg_inp[0, len(translation)]\n",
    "    # now we have to pass it through a decoder until we get a <eos> token or we exceed trans_len\n",
    "    while len(translation)<trans_len and last_token!=2: # 2 means <eos> token\n",
    "        # now we have to pass the above inputs through decoder\n",
    "        #dec_op = model.decoder(enc_op, trg_inp, dec_mask, enc_mask) \n",
    "        # now shape of decoder op will be \n",
    "        # shape of output is (1, hindi_vocab_size)\n",
    "        op = model(inp, trg_inp, enc_mask, trg_mask) # shape of op will be (max_seq_len, trg_vocab_size)\n",
    "#         print(op.shape)\n",
    "#         print(op)\n",
    "        \n",
    "        op = op.argmax(dim=1)  # shape will be (max_seq_len)  Greedy decoding\n",
    "#         print(op)\n",
    "        last_token = op[len(translation)].item()\n",
    "#         print(last_token)\n",
    "        translation.append(last_token)\n",
    "        trg_inp[0, len(translation)] = last_token  # updating the last token in the trg_inp\n",
    "        trg_mask[0, len(translation)] = 1  # setting up the mask for the current value equal to 1\n",
    "        #print(f\"DONE {len(translation)} times\")\n",
    "        \n",
    "    model.train()\n",
    "    return bpemb_hi.decode(translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873125a",
   "metadata": {
    "papermill": {
     "duration": 0.017105,
     "end_time": "2023-05-12T14:24:05.805506",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.788401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DRIVER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "575c18d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.840856Z",
     "iopub.status.busy": "2023-05-12T14:24:05.840097Z",
     "iopub.status.idle": "2023-05-12T14:24:05.845729Z",
     "shell.execute_reply": "2023-05-12T14:24:05.844829Z"
    },
    "papermill": {
     "duration": 0.025483,
     "end_time": "2023-05-12T14:24:05.847758",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.822275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "num_epochs =  30\n",
    "lr = 3e-4\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = CustomDataset(train_split, max_seq_len=64)\n",
    "test_dataset = CustomDataset(test_split, max_seq_len=64)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1326ab88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:05.882277Z",
     "iopub.status.busy": "2023-05-12T14:24:05.881544Z",
     "iopub.status.idle": "2023-05-12T14:24:10.835599Z",
     "shell.execute_reply": "2023-05-12T14:24:10.833001Z"
    },
    "papermill": {
     "duration": 4.974286,
     "end_time": "2023-05-12T14:24:10.838574",
     "exception": false,
     "start_time": "2023-05-12T14:24:05.864288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1              [-1, 64, 512]       5,120,512\n",
      "         Embedding-2              [-1, 64, 512]          32,768\n",
      "           Dropout-3              [-1, 64, 512]               0\n",
      "            Linear-4              [-1, 64, 512]         262,144\n",
      "            Linear-5              [-1, 64, 512]         262,144\n",
      "            Linear-6              [-1, 64, 512]         262,144\n",
      "            Linear-7              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-8              [-1, 64, 512]               0\n",
      "           Dropout-9              [-1, 64, 512]               0\n",
      "        LayerNorm-10              [-1, 64, 512]           1,024\n",
      "           Linear-11             [-1, 64, 2048]       1,050,624\n",
      "             ReLU-12             [-1, 64, 2048]               0\n",
      "           Linear-13              [-1, 64, 512]       1,049,088\n",
      "      feedforward-14              [-1, 64, 512]               0\n",
      "          Dropout-15              [-1, 64, 512]               0\n",
      "        LayerNorm-16              [-1, 64, 512]           1,024\n",
      "    encoder_block-17              [-1, 64, 512]               0\n",
      "           Linear-18              [-1, 64, 512]         262,144\n",
      "           Linear-19              [-1, 64, 512]         262,144\n",
      "           Linear-20              [-1, 64, 512]         262,144\n",
      "           Linear-21              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-22              [-1, 64, 512]               0\n",
      "          Dropout-23              [-1, 64, 512]               0\n",
      "        LayerNorm-24              [-1, 64, 512]           1,024\n",
      "           Linear-25             [-1, 64, 2048]       1,050,624\n",
      "             ReLU-26             [-1, 64, 2048]               0\n",
      "           Linear-27              [-1, 64, 512]       1,049,088\n",
      "      feedforward-28              [-1, 64, 512]               0\n",
      "          Dropout-29              [-1, 64, 512]               0\n",
      "        LayerNorm-30              [-1, 64, 512]           1,024\n",
      "    encoder_block-31              [-1, 64, 512]               0\n",
      "           Linear-32              [-1, 64, 512]         262,144\n",
      "           Linear-33              [-1, 64, 512]         262,144\n",
      "           Linear-34              [-1, 64, 512]         262,144\n",
      "           Linear-35              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-36              [-1, 64, 512]               0\n",
      "          Dropout-37              [-1, 64, 512]               0\n",
      "        LayerNorm-38              [-1, 64, 512]           1,024\n",
      "           Linear-39             [-1, 64, 2048]       1,050,624\n",
      "             ReLU-40             [-1, 64, 2048]               0\n",
      "           Linear-41              [-1, 64, 512]       1,049,088\n",
      "      feedforward-42              [-1, 64, 512]               0\n",
      "          Dropout-43              [-1, 64, 512]               0\n",
      "        LayerNorm-44              [-1, 64, 512]           1,024\n",
      "    encoder_block-45              [-1, 64, 512]               0\n",
      "encoder_transformer-46              [-1, 64, 512]               0\n",
      "        Embedding-47              [-1, 64, 512]       5,120,512\n",
      "        Embedding-48              [-1, 64, 512]          32,768\n",
      "          Dropout-49              [-1, 64, 512]               0\n",
      "           Linear-50              [-1, 64, 512]         262,144\n",
      "           Linear-51              [-1, 64, 512]         262,144\n",
      "           Linear-52              [-1, 64, 512]         262,144\n",
      "           Linear-53              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-54              [-1, 64, 512]               0\n",
      "          Dropout-55              [-1, 64, 512]               0\n",
      "        LayerNorm-56              [-1, 64, 512]           1,024\n",
      "           Linear-57              [-1, 64, 512]         262,144\n",
      "           Linear-58              [-1, 64, 512]         262,144\n",
      "           Linear-59              [-1, 64, 512]         262,144\n",
      "           Linear-60              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-61              [-1, 64, 512]               0\n",
      "          Dropout-62              [-1, 64, 512]               0\n",
      "        LayerNorm-63              [-1, 64, 512]           1,024\n",
      "           Linear-64             [-1, 64, 2048]       1,050,624\n",
      "             ReLU-65             [-1, 64, 2048]               0\n",
      "           Linear-66              [-1, 64, 512]       1,049,088\n",
      "      feedforward-67              [-1, 64, 512]               0\n",
      "          Dropout-68              [-1, 64, 512]               0\n",
      "        LayerNorm-69              [-1, 64, 512]           1,024\n",
      "    decoder_block-70              [-1, 64, 512]               0\n",
      "           Linear-71              [-1, 64, 512]         262,144\n",
      "           Linear-72              [-1, 64, 512]         262,144\n",
      "           Linear-73              [-1, 64, 512]         262,144\n",
      "           Linear-74              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-75              [-1, 64, 512]               0\n",
      "          Dropout-76              [-1, 64, 512]               0\n",
      "        LayerNorm-77              [-1, 64, 512]           1,024\n",
      "           Linear-78              [-1, 64, 512]         262,144\n",
      "           Linear-79              [-1, 64, 512]         262,144\n",
      "           Linear-80              [-1, 64, 512]         262,144\n",
      "           Linear-81              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-82              [-1, 64, 512]               0\n",
      "          Dropout-83              [-1, 64, 512]               0\n",
      "        LayerNorm-84              [-1, 64, 512]           1,024\n",
      "           Linear-85             [-1, 64, 2048]       1,050,624\n",
      "             ReLU-86             [-1, 64, 2048]               0\n",
      "           Linear-87              [-1, 64, 512]       1,049,088\n",
      "      feedforward-88              [-1, 64, 512]               0\n",
      "          Dropout-89              [-1, 64, 512]               0\n",
      "        LayerNorm-90              [-1, 64, 512]           1,024\n",
      "    decoder_block-91              [-1, 64, 512]               0\n",
      "           Linear-92              [-1, 64, 512]         262,144\n",
      "           Linear-93              [-1, 64, 512]         262,144\n",
      "           Linear-94              [-1, 64, 512]         262,144\n",
      "           Linear-95              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-96              [-1, 64, 512]               0\n",
      "          Dropout-97              [-1, 64, 512]               0\n",
      "        LayerNorm-98              [-1, 64, 512]           1,024\n",
      "           Linear-99              [-1, 64, 512]         262,144\n",
      "          Linear-100              [-1, 64, 512]         262,144\n",
      "          Linear-101              [-1, 64, 512]         262,144\n",
      "          Linear-102              [-1, 64, 512]         262,656\n",
      "MultiHeadSelfAttention-103              [-1, 64, 512]               0\n",
      "         Dropout-104              [-1, 64, 512]               0\n",
      "       LayerNorm-105              [-1, 64, 512]           1,024\n",
      "          Linear-106             [-1, 64, 2048]       1,050,624\n",
      "            ReLU-107             [-1, 64, 2048]               0\n",
      "          Linear-108              [-1, 64, 512]       1,049,088\n",
      "     feedforward-109              [-1, 64, 512]               0\n",
      "         Dropout-110              [-1, 64, 512]               0\n",
      "       LayerNorm-111              [-1, 64, 512]           1,024\n",
      "   decoder_block-112              [-1, 64, 512]               0\n",
      "decoder_transformer-113              [-1, 64, 512]               0\n",
      "          Linear-114                [-1, 10001]       5,130,513\n",
      "================================================================\n",
      "Total params: 37,492,497\n",
      "Trainable params: 37,492,497\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 37.33\n",
      "Params size (MB): 143.02\n",
      "Estimated Total Size (MB): 180.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model hyperparameters\n",
    "num_blocks = 3\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "hidden_dim = 4*d_model\n",
    "src_vocab_size = bpemb_en.vocab_size + 1 # +1 due to padding token\n",
    "trg_vocab_size = bpemb_hi.vocab_size + 1 # +1 due to padding token\n",
    "max_seq_len = 64\n",
    "\n",
    "# testing the model\n",
    "model = transformer(num_blocks, d_model, num_heads, hidden_dim, src_vocab_size, \n",
    "                              trg_vocab_size, max_seq_len).to(device)\n",
    "summary(model, [(max_seq_len, ), (max_seq_len ,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9baeb5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:10.874175Z",
     "iopub.status.busy": "2023-05-12T14:24:10.873867Z",
     "iopub.status.idle": "2023-05-12T14:24:10.926560Z",
     "shell.execute_reply": "2023-05-12T14:24:10.925431Z"
    },
    "papermill": {
     "duration": 0.072713,
     "end_time": "2023-05-12T14:24:10.928662",
     "exception": false,
     "start_time": "2023-05-12T14:24:10.855949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Sentence ['offerings of sacrifice, bali is the most important aspect of rural life. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ ']\n",
      "\n",
      "Original Hindi Sentence ['बलि ग्रामीन जीवन का अत्यंत महत्वपूर्ण पहलू हैं। ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ ']\n",
      "\n",
      "Predicted Sentence ['out', ')', '$', 'अम्ब', 'formation', 'गोद', 'गिर', 'ब्ल', 'श्री', 'कैथ', 'बस', 'पुरी', 'नाटक', 'ासा', 'िरण', 'पुरी', 'नाश', 'बै', 'वाई', 'नर', 'इनसे', 'कोई', 'स्वाभाविक', 'तौर', 'यी', 'स्लिम', 'तौर', 'द्दी', 'मार्ग', 'बिना', 'फरवरी', 'बरी', 'हीत', 'ासा', 'काव', 'बरी', 'स्लिम', 'विद्वानों', 'उस', 'reg', 'हा', 'ासा', 'हारी', 'हा', 'कोई', 'पुरी', 'ऋषि', 'टीय', 'संबंधित', 'मजबूर', 'ासा', 'स्कॉटलैंड', 'reg', 'त्मा', 'मजबूर', 'नज़र', 'सैद्ध', 'कोई', 'चलकर', 'चलकर', 'मजबूर', 'बंध', 'ug', 'कोई']\n"
     ]
    }
   ],
   "source": [
    "# let us test our model on some actual input to check it doesn't break\n",
    "for _, batch in enumerate(train_loader):\n",
    "    src = batch[0][0].unsqueeze(0).to(device)\n",
    "    trg_inp = batch[1][0].unsqueeze(0).to(device)\n",
    "    trg_op = batch[2][0].unsqueeze(0).to(device)\n",
    "    src_pad_mask = batch[3][0].unsqueeze(0).to(device)\n",
    "    trg_pad_mask = batch[4][0].unsqueeze(0).to(device)\n",
    "    break\n",
    "    \n",
    "#print(f'{src}\\n\\n{src_pad_mask}\\n\\n{trg}\\n\\n{trg_pad_mask}')\n",
    "op = model(src, trg_inp, src_pad_mask, trg_pad_mask)\n",
    "op = nn.Softmax(dim=-1)(op)\n",
    "out = torch.max(op, dim=-1).indices\n",
    "print(f'Original English Sentence {bpemb_en.decode(src.tolist())}\\n\\nOriginal Hindi Sentence {bpemb_hi.decode(trg_inp.tolist())}\\n\\nPredicted Sentence {bpemb_hi.decode(out.tolist())}')\n",
    "# ?? are because of the padding tokens we can easily remove them when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3ce9163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:10.964345Z",
     "iopub.status.busy": "2023-05-12T14:24:10.963450Z",
     "iopub.status.idle": "2023-05-12T14:24:11.325239Z",
     "shell.execute_reply": "2023-05-12T14:24:11.323760Z"
    },
    "papermill": {
     "duration": 0.381988,
     "end_time": "2023-05-12T14:24:11.327520",
     "exception": false,
     "start_time": "2023-05-12T14:24:10.945532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [ 0.9173,  0.6506, -0.2987,  ...,  0.2413, -0.6024,  0.3187],\n",
      "        [ 0.5707,  0.1652, -0.3550,  ..., -0.3393, -1.0696,  0.3579],\n",
      "        ...,\n",
      "        [ 0.3740, -0.1012, -0.1618,  ...,  0.1387, -0.2267,  0.4021],\n",
      "        [ 1.1387, -0.3264, -0.5040,  ...,  0.6843, -0.2320, -0.5468],\n",
      "        [ 0.2644, -0.1942, -0.1974,  ..., -0.0265, -0.7764,  0.2939]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 2685, 8800, 2575, 3775, 5174, 2338, 1490, 2065, 2065, 7514, 2338,\n",
      "        6537, 2575, 5174, 2575, 3701, 1062, 9188, 7691, 6318, 1062, 2575, 1062,\n",
      "        3775, 8213, 1062, 7323, 2575, 1676, 2575, 2065, 2575, 2575, 2575, 1062,\n",
      "        2575, 2235, 2065, 9503, 2575, 1062, 3008,  151, 8337, 2575, 1062, 2575,\n",
      "         562, 9903, 2575, 7742, 2575, 3775, 3051, 3775, 3008, 2065, 2575, 1490,\n",
      "        5060,  197, 5174,  662], device='cuda:0')\n",
      "1932\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [ 0.6154,  0.1243, -0.3396,  ..., -0.5276, -1.1801,  0.5650],\n",
      "        ...,\n",
      "        [ 0.4528, -0.0657, -0.1996,  ..., -0.0208, -0.3060,  0.6332],\n",
      "        [ 1.2199, -0.2492, -0.6128,  ...,  0.3849, -0.5102, -0.2268],\n",
      "        [ 0.3887, -0.1368, -0.2709,  ..., -0.1729, -0.8822,  0.5274]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 2575, 1297, 4428, 9977, 2575, 8854, 4428, 7514, 7261,\n",
      "        6537, 2575, 4774, 2575, 7240, 1062, 1505, 2133, 3008, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7085, 2575, 1676, 3434, 2065, 9212, 2575, 7013, 2133,\n",
      "        2575, 2235,  197, 6675, 2575, 1062, 3008,  151, 1511, 2065, 1062, 2575,\n",
      "         562, 9264, 2575, 4341, 2575, 2575, 5878, 1062, 3008, 2065, 2796, 4757,\n",
      "        1577,  197, 5174,  662], device='cuda:0')\n",
      "1220\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.3536,  0.0669, -0.0125,  ..., -0.1746, -0.3656,  0.7754],\n",
      "        [ 1.0258, -0.1855, -0.3765,  ...,  0.3114, -0.4876, -0.1402],\n",
      "        [ 0.2540, -0.1006, -0.0904,  ..., -0.2775, -0.9000,  0.6498]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 7514,  197, 4341,  197, 2575, 8854, 7514, 7514, 3080,\n",
      "        6537, 2575, 6537, 2575, 7240, 1062, 1505, 2133,  197, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 2271, 9212, 2575, 7013, 2133,\n",
      "        2133, 2235,  197,  197,  792, 1062, 2133, 3080, 1511, 5551, 1062, 2575,\n",
      "         562, 1511, 2575, 4341, 8426, 2575, 1505, 5077, 7357, 6537, 6537, 6136,\n",
      "        4341,  197, 5174,  662], device='cuda:0')\n",
      "8800\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.3557,  0.0094, -0.1231,  ..., -0.3717, -0.3380,  0.7966],\n",
      "        [ 1.1086, -0.2544, -0.5434,  ...,  0.1356, -0.4185, -0.2007],\n",
      "        [ 0.3804, -0.1316, -0.3043,  ..., -0.5087, -0.8697,  0.6378]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735,  197, 5174, 8946, 2575, 8854, 7514, 7514, 2338,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 1505, 2133, 3008, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 6336, 1676, 3434,  262, 9212, 2575, 7013, 1062,\n",
      "        8426, 2235,  197, 6112, 5631, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        3008, 1511, 1062, 4341, 8426, 2575, 7280, 1062, 3008, 8426, 6537, 5174,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "5735\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.3687,  0.0740, -0.0302,  ..., -0.3300, -0.2478,  0.7763],\n",
      "        [ 1.1781, -0.1412, -0.4703,  ...,  0.1434, -0.3340, -0.2133],\n",
      "        [ 0.3957, -0.0333, -0.2134,  ..., -0.4732, -0.8138,  0.6346]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 4428,  197, 2575, 8854, 7514, 7514, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 1505, 5993, 6318, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 6336, 1676, 6537,  262, 9212, 2575, 7013, 1062,\n",
      "        2133, 2235,  197, 5560,  792, 1062,  197,  151, 1511, 5551, 1062, 1380,\n",
      "        3008, 1511, 1062, 4341, 8426, 2575, 7514, 1062, 7357, 6537, 6537, 5174,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "9487\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4664,  0.1311, -0.0203,  ..., -0.3681, -0.1286,  0.8430],\n",
      "        [ 1.3035, -0.1054, -0.4444,  ...,  0.0907, -0.1856, -0.1953],\n",
      "        [ 0.5038, -0.0050, -0.2258,  ..., -0.5365, -0.7199,  0.6763]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 2338, 2575,   67, 7514, 7514, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 1505, 2636, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434,  262, 9212, 2575, 4135, 1062,\n",
      "        2133, 2235,  197, 6112, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        1062, 1511, 1062, 4341, 8426, 2575, 7280, 1062, 7357, 6537, 6537, 1062,\n",
      "        1062,  197, 1062, 1062], device='cuda:0')\n",
      "2093\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4272,  0.1672,  0.0411,  ..., -0.4554, -0.1319,  0.7579],\n",
      "        [ 1.2886, -0.0627, -0.4140,  ...,  0.0254, -0.1585, -0.2848],\n",
      "        [ 0.4501,  0.0030, -0.1809,  ..., -0.6341, -0.7021,  0.5928]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 2575, 5964, 7514, 7514, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 1505, 2636, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434,  262, 9212, 2575, 4135, 1062,\n",
      "        2133, 2235, 3080, 6001, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1062, 1062, 4341, 8426, 2575, 1505, 1062, 7357, 6537, 6537, 1062,\n",
      "        1062,  197, 1062, 1062], device='cuda:0')\n",
      "6365\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4413,  0.2488,  0.0550,  ..., -0.4404, -0.0969,  0.7766],\n",
      "        [ 1.2789, -0.0016, -0.4027,  ...,  0.0322, -0.1058, -0.3052],\n",
      "        [ 0.4459,  0.0823, -0.1801,  ..., -0.6360, -0.6917,  0.6191]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225, 1121, 7514, 7514, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 1505, 2636, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 4135, 1062,\n",
      "        2133, 2235, 2133, 6001, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 2575, 3434, 1062, 7357, 8426, 6537, 6136,\n",
      "        1062,  197, 1062, 1062], device='cuda:0')\n",
      "1225\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4512,  0.2213,  0.0303,  ..., -0.3769, -0.1307,  0.7124],\n",
      "        [ 1.2849, -0.0159, -0.4450,  ...,  0.0779, -0.1106, -0.3556],\n",
      "        [ 0.4524,  0.0729, -0.2415,  ..., -0.5976, -0.7149,  0.6072]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 7514, 7514, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 1505, 2133, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 4135, 1062,\n",
      "        2133, 2235, 2133, 6001, 2133, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        1062, 1511, 1062, 4341, 8426, 2575, 2133, 1062, 7357, 6537, 6537, 6136,\n",
      "        1062,  197, 1062, 1062], device='cuda:0')\n",
      "29\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4274,  0.2116,  0.0799,  ..., -0.2973, -0.1017,  0.6813],\n",
      "        [ 1.2754, -0.0673, -0.4282,  ...,  0.1496, -0.1122, -0.3930],\n",
      "        [ 0.4400,  0.0270, -0.1950,  ..., -0.5518, -0.7264,  0.5957]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7514, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 4774, 2133, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 2575, 2575, 4135, 1062,\n",
      "        2133, 2235, 2133, 6112, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1062, 1062, 4341, 8426, 3775, 2133, 1062, 7357, 6537, 6537, 1062,\n",
      "        1062, 2923, 1062,  662], device='cuda:0')\n",
      "1144\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5093,  0.2115,  0.1061,  ..., -0.3200, -0.1147,  0.6428],\n",
      "        [ 1.3299, -0.0530, -0.4154,  ...,  0.1484, -0.1280, -0.4483],\n",
      "        [ 0.4688,  0.0330, -0.1729,  ..., -0.5595, -0.7158,  0.5325]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 7261,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 4774, 2636, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 2575, 2575, 4135, 1062,\n",
      "        2133, 2235, 8756, 6112, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 3434, 1062, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "7322\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5204,  0.2134,  0.0833,  ..., -0.3687, -0.1392,  0.6512],\n",
      "        [ 1.3309, -0.0827, -0.4203,  ...,  0.1219, -0.1500, -0.4360],\n",
      "        [ 0.4653,  0.0178, -0.1914,  ..., -0.5720, -0.7599,  0.5716]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        6537, 2575, 1062, 2575, 9073, 1062, 4774, 2133, 1062, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7323, 2575, 1676, 3434, 4774, 2575, 2575, 4135, 1062,\n",
      "        2133, 2235, 8756, 6001, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 2133, 1062, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "4100\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5435,  0.2561,  0.1169,  ..., -0.3243, -0.1516,  0.6473],\n",
      "        [ 1.3636, -0.0676, -0.3969,  ...,  0.1636, -0.1683, -0.4361],\n",
      "        [ 0.4769,  0.0471, -0.1651,  ..., -0.5298, -0.7868,  0.5527]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 2575, 1062, 2575, 9073, 1062, 4774, 2636, 2253, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 2575, 2575, 2575, 1062,\n",
      "        2923, 2235, 8756, 6001, 2575, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 3434, 1062, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "2691\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5395,  0.2445,  0.1089,  ..., -0.3398, -0.1276,  0.6409],\n",
      "        [ 1.3635, -0.0962, -0.3914,  ...,  0.1347, -0.1512, -0.4580],\n",
      "        [ 0.4763,  0.0134, -0.1634,  ..., -0.5545, -0.7653,  0.5343]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 4774, 2575, 9073, 1062, 4774, 2636, 1946, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 4135, 6976,\n",
      "        2923, 2235, 8756, 6112,  792, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 3434, 1062, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "4888\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5161,  0.2321,  0.0644,  ..., -0.3652, -0.1259,  0.6175],\n",
      "        [ 1.3561, -0.1050, -0.4270,  ...,  0.1113, -0.1265, -0.4813],\n",
      "        [ 0.4499, -0.0066, -0.1823,  ..., -0.5707, -0.7466,  0.4965]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 2575, 9073, 1062, 4774, 2636, 2253, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 2575, 2575, 2575, 6976,\n",
      "        2923, 2235, 8756, 6112,  792, 1062, 2133,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 3434, 5077, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 1062,  662], device='cuda:0')\n",
      "9794\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5262,  0.2537,  0.0664,  ..., -0.3467, -0.1311,  0.6277],\n",
      "        [ 1.3855, -0.0650, -0.4287,  ...,  0.1347, -0.1179, -0.4800],\n",
      "        [ 0.4708,  0.0353, -0.1783,  ..., -0.5572, -0.7362,  0.5100]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 9073, 1062, 4774, 2636, 6318, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 2575, 2575, 2575, 6976,\n",
      "        2923, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 5878, 5077, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 5551,  662], device='cuda:0')\n",
      "8181\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5451,  0.2567,  0.0999,  ..., -0.3294, -0.1122,  0.6136],\n",
      "        [ 1.4037, -0.0491, -0.3991,  ...,  0.1372, -0.1076, -0.4803],\n",
      "        [ 0.4848,  0.0438, -0.1399,  ..., -0.5435, -0.7260,  0.5219]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 1062, 4774, 2636, 6318, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9859, 2575, 2575, 6976,\n",
      "        2923, 2235, 8756, 6112,  792, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 4528,  622, 7357, 4774, 6537, 9271,\n",
      "        1062,  197, 5551,  662], device='cuda:0')\n",
      "6455\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5637,  0.2548,  0.1430,  ..., -0.3020, -0.1191,  0.6519],\n",
      "        [ 1.4186, -0.0570, -0.3504,  ...,  0.1558, -0.0979, -0.4359],\n",
      "        [ 0.4875,  0.0370, -0.1002,  ..., -0.5262, -0.7224,  0.5727]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 4774, 2636, 6318, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 9264, 1062, 6328, 8426, 3775, 4528, 1062, 7357, 4774, 6537, 9271,\n",
      "        1062,  197, 5551,  662], device='cuda:0')\n",
      "4451\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5848,  0.2587,  0.1800,  ..., -0.3062, -0.1606,  0.6801],\n",
      "        [ 1.4350, -0.0468, -0.3147,  ...,  0.1635, -0.1338, -0.3980],\n",
      "        [ 0.5063,  0.0331, -0.0489,  ..., -0.5219, -0.7579,  0.5981]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 2636, 6318, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 6112,  792, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 6328, 8426, 3775, 2133,  622, 7357, 6537, 6537, 9271,\n",
      "        6537,  197, 5551,  662], device='cuda:0')\n",
      "7487\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.6010,  0.2520,  0.1563,  ..., -0.3248, -0.1612,  0.6844],\n",
      "        [ 1.4502, -0.0719, -0.3313,  ...,  0.1541, -0.1174, -0.3978],\n",
      "        [ 0.5151,  0.0131, -0.0734,  ..., -0.5362, -0.7541,  0.5955]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 6318, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 5560,  792, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 6328, 8426, 3775, 5878, 1062, 7357, 6537, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "4938\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5852,  0.2456,  0.1653,  ..., -0.3221, -0.1654,  0.6846],\n",
      "        [ 1.4305, -0.0735, -0.3188,  ...,  0.1528, -0.1321, -0.4032],\n",
      "        [ 0.4900,  0.0126, -0.0645,  ..., -0.5448, -0.7694,  0.5895]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 1062, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 5560, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 6328, 8426, 3775, 5878, 1062, 7357, 6537, 6537, 9271,\n",
      "        6537,  197, 8426,  662], device='cuda:0')\n",
      "9435\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5727,  0.2650,  0.1568,  ..., -0.3237, -0.1795,  0.6795],\n",
      "        [ 1.4337, -0.0594, -0.3409,  ...,  0.1532, -0.1369, -0.4100],\n",
      "        [ 0.4874,  0.0298, -0.0827,  ..., -0.5439, -0.7921,  0.5798]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 2575, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 5560,  792, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 5878, 1062, 7357, 6537, 6537, 9271,\n",
      "        6537,  197, 8426,  662], device='cuda:0')\n",
      "5165\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5922,  0.2552,  0.1664,  ..., -0.3168, -0.1765,  0.6955],\n",
      "        [ 1.4499, -0.0610, -0.3357,  ...,  0.1742, -0.1336, -0.3908],\n",
      "        [ 0.5039,  0.0194, -0.0749,  ..., -0.5419, -0.7811,  0.5892]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 1062,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 6328, 8426, 3775, 7514,  622, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "1295\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.6114,  0.2611,  0.1541,  ..., -0.3264, -0.1792,  0.7083],\n",
      "        [ 1.4709, -0.0598, -0.3398,  ...,  0.1723, -0.1397, -0.3779],\n",
      "        [ 0.5293,  0.0157, -0.0802,  ..., -0.5439, -0.7797,  0.5947]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "         572, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 5560,  792, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "4509\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5889,  0.2730,  0.1440,  ..., -0.3378, -0.1728,  0.7202],\n",
      "        [ 1.4521, -0.0461, -0.3514,  ...,  0.1717, -0.1276, -0.3652],\n",
      "        [ 0.5162,  0.0342, -0.0892,  ..., -0.5554, -0.7754,  0.6116]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 8213, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 5560, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "6769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5741,  0.2709,  0.1377,  ..., -0.3167, -0.1663,  0.7388],\n",
      "        [ 1.4408, -0.0433, -0.3484,  ...,  0.1912, -0.1355, -0.3490],\n",
      "        [ 0.5034,  0.0385, -0.0888,  ..., -0.5409, -0.7816,  0.6227]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 1062, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 2636,  662], device='cuda:0')\n",
      "4678\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5686,  0.2494,  0.1394,  ..., -0.3247, -0.1620,  0.7471],\n",
      "        [ 1.4377, -0.0641, -0.3456,  ...,  0.1881, -0.1169, -0.3332],\n",
      "        [ 0.5056,  0.0231, -0.0824,  ..., -0.5384, -0.7720,  0.6311]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 7514, 2575, 1676, 3434, 4774, 9212, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 5560, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 2636,  662], device='cuda:0')\n",
      "4235\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5582,  0.2529,  0.1436,  ..., -0.3527, -0.1495,  0.7358],\n",
      "        [ 1.4225, -0.0604, -0.3377,  ...,  0.1570, -0.1065, -0.3367],\n",
      "        [ 0.5009,  0.0188, -0.0856,  ..., -0.5576, -0.7576,  0.6199]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 2575, 1676, 3434, 4774, 9859, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 2636,  662], device='cuda:0')\n",
      "6797\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5512,  0.2209,  0.1456,  ..., -0.3663, -0.1484,  0.7229],\n",
      "        [ 1.4238, -0.0668, -0.3345,  ...,  0.1470, -0.1080, -0.3464],\n",
      "        [ 0.5008,  0.0160, -0.0795,  ..., -0.5653, -0.7630,  0.6139]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 3434, 4774, 9859, 2575, 2575, 6976,\n",
      "        8426, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "9864\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5315,  0.2227,  0.1389,  ..., -0.3736, -0.1428,  0.7254],\n",
      "        [ 1.3964, -0.0766, -0.3397,  ...,  0.1463, -0.1017, -0.3540],\n",
      "        [ 0.4869,  0.0039, -0.0840,  ..., -0.5729, -0.7535,  0.6159]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 3434, 4774, 9859, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "1676\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5355,  0.2124,  0.1616,  ..., -0.3756, -0.1469,  0.7370],\n",
      "        [ 1.3985, -0.0794, -0.3195,  ...,  0.1461, -0.1034, -0.3419],\n",
      "        [ 0.4853,  0.0055, -0.0626,  ..., -0.5781, -0.7585,  0.6282]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 4774, 9859, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "7603\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5424,  0.2330,  0.1424,  ..., -0.3746, -0.1241,  0.7430],\n",
      "        [ 1.4209, -0.0531, -0.3283,  ...,  0.1478, -0.0899, -0.3329],\n",
      "        [ 0.4945,  0.0216, -0.0619,  ..., -0.5723, -0.7315,  0.6264]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 9859, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "1433\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5194,  0.2265,  0.1373,  ..., -0.3952, -0.1241,  0.7262],\n",
      "        [ 1.4011, -0.0655, -0.3343,  ...,  0.1364, -0.0935, -0.3473],\n",
      "        [ 0.4720,  0.0101, -0.0656,  ..., -0.5814, -0.7386,  0.6152]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 2575, 2575, 6976,\n",
      "        8426, 2235, 7514, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "4633\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5139,  0.2339,  0.1372,  ..., -0.4040, -0.1243,  0.7138],\n",
      "        [ 1.3901, -0.0570, -0.3363,  ...,  0.1243, -0.0940, -0.3634],\n",
      "        [ 0.4662,  0.0147, -0.0638,  ..., -0.5924, -0.7376,  0.6028]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 2575,  141,\n",
      "        8426, 2235, 7514, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 6537, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "4525\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5273,  0.2421,  0.1539,  ..., -0.3925, -0.1215,  0.7258],\n",
      "        [ 1.4068, -0.0444, -0.3255,  ...,  0.1314, -0.0871, -0.3569],\n",
      "        [ 0.4885,  0.0358, -0.0509,  ..., -0.5824, -0.7398,  0.6104]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882,  141,\n",
      "        8426, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514, 1062, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "7882\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5219,  0.2405,  0.1481,  ..., -0.3994, -0.1128,  0.7423],\n",
      "        [ 1.4023, -0.0378, -0.3292,  ...,  0.1318, -0.0841, -0.3467],\n",
      "        [ 0.4825,  0.0486, -0.0549,  ..., -0.5933, -0.7334,  0.6279]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        8426, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  135, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "6116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5129,  0.2385,  0.1324,  ..., -0.4102, -0.1233,  0.7374],\n",
      "        [ 1.3873, -0.0464, -0.3597,  ...,  0.1126, -0.1024, -0.3647],\n",
      "        [ 0.4758,  0.0373, -0.0699,  ..., -0.6124, -0.7599,  0.6243]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 2235, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  135, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "2007\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5097,  0.2389,  0.1346,  ..., -0.4285, -0.1282,  0.7503],\n",
      "        [ 1.3900, -0.0486, -0.3470,  ...,  0.0925, -0.1142, -0.3565],\n",
      "        [ 0.4776,  0.0335, -0.0577,  ..., -0.6318, -0.7683,  0.6339]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 8756, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  135, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "4020\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5115,  0.2298,  0.1213,  ..., -0.4266, -0.1392,  0.7508],\n",
      "        [ 1.3932, -0.0571, -0.3555,  ...,  0.0929, -0.1252, -0.3533],\n",
      "        [ 0.4775,  0.0284, -0.0699,  ..., -0.6329, -0.7739,  0.6373]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6112, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514,  135, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "5583\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5001,  0.2212,  0.1180,  ..., -0.4284, -0.1205,  0.7504],\n",
      "        [ 1.3857, -0.0602, -0.3528,  ...,  0.0903, -0.1096, -0.3491],\n",
      "        [ 0.4641,  0.0226, -0.0712,  ..., -0.6284, -0.7523,  0.6411]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 2575, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  135, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "6675\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5048,  0.2066,  0.1058,  ..., -0.4345, -0.1335,  0.7401],\n",
      "        [ 1.3861, -0.0719, -0.3606,  ...,  0.0848, -0.1190, -0.3627],\n",
      "        [ 0.4643,  0.0145, -0.0761,  ..., -0.6329, -0.7594,  0.6288]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1062, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "7156\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5130,  0.1982,  0.1047,  ..., -0.4272, -0.1166,  0.7396],\n",
      "        [ 1.3926, -0.0799, -0.3620,  ...,  0.0886, -0.1112, -0.3640],\n",
      "        [ 0.4702,  0.0082, -0.0867,  ..., -0.6260, -0.7447,  0.6311]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 4281,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "1745\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5241,  0.1986,  0.1181,  ..., -0.4260, -0.1171,  0.7240],\n",
      "        [ 1.3913, -0.0730, -0.3548,  ...,  0.0931, -0.1028, -0.3828],\n",
      "        [ 0.4722,  0.0146, -0.0770,  ..., -0.6199, -0.7370,  0.6141]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564,  151, 1511, 5551, 1062, 2575,\n",
      "        2575, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "9564\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.5269,  0.1933,  0.1172,  ..., -0.4193, -0.1227,  0.7219],\n",
      "        [ 1.3975, -0.0820, -0.3539,  ...,  0.1004, -0.1089, -0.3832],\n",
      "        [ 0.4739,  0.0079, -0.0758,  ..., -0.6091, -0.7393,  0.6140]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 1511, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "1702\n",
      "tensor([[ 4.0258e-01, -1.0616e+00,  3.5170e-01,  ...,  1.3255e-01,\n",
      "         -1.2927e+00,  1.6296e-02],\n",
      "        [-1.0344e-01,  3.7465e-01,  1.1886e-01,  ...,  4.5237e-01,\n",
      "          2.3466e-01,  1.3635e-01],\n",
      "        [-3.6814e-01, -1.3871e-01, -3.7076e-01,  ..., -4.5666e-01,\n",
      "         -6.3760e-01,  7.2731e-02],\n",
      "        ...,\n",
      "        [ 5.1063e-01,  1.8841e-01,  1.1631e-01,  ..., -4.0864e-01,\n",
      "         -1.1668e-01,  7.1360e-01],\n",
      "        [ 1.3816e+00, -8.7617e-02, -3.5249e-01,  ...,  1.0576e-01,\n",
      "         -1.0488e-01, -3.9271e-01],\n",
      "        [ 4.6091e-01,  1.2610e-03, -7.6714e-02,  ..., -6.0261e-01,\n",
      "         -7.4085e-01,  6.0700e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 6049, 5551, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "6049\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4964,  0.1928,  0.0960,  ..., -0.4130, -0.1127,  0.7125],\n",
      "        [ 1.3745, -0.0890, -0.3658,  ...,  0.0987, -0.1035, -0.4008],\n",
      "        [ 0.4522,  0.0057, -0.0887,  ..., -0.6057, -0.7429,  0.6028]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 6049, 7127, 1062, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "7127\n",
      "tensor([[ 4.0258e-01, -1.0616e+00,  3.5170e-01,  ...,  1.3255e-01,\n",
      "         -1.2927e+00,  1.6296e-02],\n",
      "        [-1.0344e-01,  3.7465e-01,  1.1886e-01,  ...,  4.5237e-01,\n",
      "          2.3466e-01,  1.3635e-01],\n",
      "        [-3.6814e-01, -1.3871e-01, -3.7076e-01,  ..., -4.5666e-01,\n",
      "         -6.3760e-01,  7.2731e-02],\n",
      "        ...,\n",
      "        [ 4.9957e-01,  1.9048e-01,  1.0623e-01,  ..., -4.1686e-01,\n",
      "         -1.0691e-01,  7.1053e-01],\n",
      "        [ 1.3797e+00, -9.3369e-02, -3.5149e-01,  ...,  9.5077e-02,\n",
      "         -9.8218e-02, -3.9692e-01],\n",
      "        [ 4.5082e-01, -4.5484e-04, -7.6397e-02,  ..., -6.1298e-01,\n",
      "         -7.3188e-01,  6.0146e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 6049, 7127,  634, 2575,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "634\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4997,  0.1922,  0.1116,  ..., -0.4190, -0.1170,  0.7146],\n",
      "        [ 1.3821, -0.0933, -0.3509,  ...,  0.0926, -0.1047, -0.3912],\n",
      "        [ 0.4515,  0.0047, -0.0745,  ..., -0.6141, -0.7360,  0.6062]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 6049, 7127,  634, 1387,\n",
      "        1930, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "1387\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4961,  0.1913,  0.1161,  ..., -0.4069, -0.1295,  0.7069],\n",
      "        [ 1.3799, -0.0902, -0.3424,  ...,  0.1052, -0.1108, -0.4014],\n",
      "        [ 0.4477,  0.0047, -0.0667,  ..., -0.6007, -0.7384,  0.5939]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 6049, 7127,  634, 1387,\n",
      "        8091, 1511, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        1062,  197, 8426,  662], device='cuda:0')\n",
      "8091\n",
      "tensor([[ 0.4026, -1.0616,  0.3517,  ...,  0.1325, -1.2927,  0.0163],\n",
      "        [-0.1034,  0.3746,  0.1189,  ...,  0.4524,  0.2347,  0.1364],\n",
      "        [-0.3681, -0.1387, -0.3708,  ..., -0.4567, -0.6376,  0.0727],\n",
      "        ...,\n",
      "        [ 0.4963,  0.1898,  0.1238,  ..., -0.4025, -0.1417,  0.7347],\n",
      "        [ 1.3868, -0.0907, -0.3350,  ...,  0.1025, -0.1236, -0.3701],\n",
      "        [ 0.4509,  0.0072, -0.0545,  ..., -0.6023, -0.7514,  0.6232]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1932, 1220, 8800, 5735, 9487, 2093, 6365, 1225,   29, 1144, 7322, 4100,\n",
      "        2691, 4888, 9794, 8181, 6455, 4451, 7487, 4938, 9435, 5165, 1295, 4509,\n",
      "        6769, 4678, 4235, 6797, 9864, 1676, 7603, 1433, 4633, 4525, 7882, 6116,\n",
      "        2007, 4020, 5583, 6675, 7156, 1745, 9564, 1702, 6049, 7127,  634, 1387,\n",
      "        8091, 5583, 1062, 4341, 8426, 3775, 7514,  622, 7357, 8426, 6537, 9271,\n",
      "        2636,  197, 8426,  662], device='cuda:0')\n",
      "5583\n",
      "['निवास', 'ab', 'फिया', 'ोरो', '*(0)', 'दिए', 'उतर', 'ाया', 'ल', 'पुन', '....', 'पहुँ', 'लड़', 'आहार', 'बुक', 'उपयोगिता', 'तू', 'श्री', 'हॉकी', 'कंप', 'ानंद', 'कृत्रिम', 'उदाहरण', 'सीख', 'नांक', 'मुक', 'ङ्', 'िके', 'n', 'बिना', 'ied', 'सार', 'आकर्षित', 'अनिवार्य', 'काव', 'de', 'ड़क', 'मानसिक', 'जड़', 'भूषण', 'सनीय', '0000)', 'सक्', 'ंग्ल', 'ुख', 'fil', 'म्प', 'नीय', 'effect', 'जड़']\n"
     ]
    }
   ],
   "source": [
    "# exampel of translated sentence\n",
    "text = 'the aluminium corporation of india, came into existence after the war.'\n",
    "print(translate_sentence(text, model, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "291354cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:11.363616Z",
     "iopub.status.busy": "2023-05-12T14:24:11.363301Z",
     "iopub.status.idle": "2023-05-12T14:24:11.369852Z",
     "shell.execute_reply": "2023-05-12T14:24:11.368955Z"
    },
    "papermill": {
     "duration": 0.026781,
     "end_time": "2023-05-12T14:24:11.371746",
     "exception": false,
     "start_time": "2023-05-12T14:24:11.344965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setups\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0) \n",
    "scaler = torch.cuda.amp.GradScaler() \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "losses = []\n",
    "# for inference\n",
    "# sentences = ['another plant, the aluminium corporation of india, came into existence after the war.', \n",
    "#              'He is doing very good these days', 'this guy is totally mad', 'what were you saying that day?']\n",
    "sentences = [\"So what happened on this day?\", \n",
    "             'Allow all sites to track my physical location', \n",
    "             'India is a democratic country', \n",
    "             \"In 1898, Sarojini Naidu became the life - partner of Dr. Govindarajulu Naidu.\", \n",
    "             'Is this a good time?',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved model\n",
    "model.load_state_dict(torch.load(\"my_checkpoint (1).pth\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9da86b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:11.407106Z",
     "iopub.status.busy": "2023-05-12T14:24:11.406359Z",
     "iopub.status.idle": "2023-05-12T14:24:11.411372Z",
     "shell.execute_reply": "2023-05-12T14:24:11.410564Z"
    },
    "papermill": {
     "duration": 0.024696,
     "end_time": "2023-05-12T14:24:11.413292",
     "exception": false,
     "start_time": "2023-05-12T14:24:11.388596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taking the test sentences for checking how good the model is trained\n",
    "def infer(sentences, model, max_seq_len, beam_search = None):\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(f\"Example {idx+1}:\\n{sentence}\\n{' '.join(translate_sentence(sentence, model, max_seq_len, beam_search = beam_search))}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "92118fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:11.448578Z",
     "iopub.status.busy": "2023-05-12T14:24:11.448315Z",
     "iopub.status.idle": "2023-05-12T14:24:12.896091Z",
     "shell.execute_reply": "2023-05-12T14:24:12.894578Z"
    },
    "papermill": {
     "duration": 1.468068,
     "end_time": "2023-05-12T14:24:12.898343",
     "exception": false,
     "start_time": "2023-05-12T14:24:11.430275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "So what happened on this day?\n",
      "तो इस दिन क्या हो गया है ? \n",
      "\n",
      "\n",
      "Example 2:\n",
      "Allow all sites to track my physical location\n",
      "सभी साइट ों को मेरे प्रति िको र्म िको स्थान लेने में असफल \n",
      "\n",
      "\n",
      "Example 3:\n",
      "India is a democratic country\n",
      "भारत एक ग्राम देश है । \n",
      "\n",
      "\n",
      "Example 4:\n",
      "In 1898, Sarojini Naidu became the life - partner of Dr. Govindarajulu Naidu.\n",
      "0000 में सर ोज िनी नाय डू डा . गोवि ंद राज ुल ू नाय डू की जीवन - संग िनी बनी ं । \n",
      "\n",
      "\n",
      "Example 5:\n",
      "Is this a good time?\n",
      "क्या यह एक समय है ? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "infer(sentences, model, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f20c11eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:24:12.934776Z",
     "iopub.status.busy": "2023-05-12T14:24:12.934483Z",
     "iopub.status.idle": "2023-05-12T14:43:03.806757Z",
     "shell.execute_reply": "2023-05-12T14:43:03.805296Z"
    },
    "papermill": {
     "duration": 1130.89279,
     "end_time": "2023-05-12T14:43:03.808868",
     "exception": false,
     "start_time": "2023-05-12T14:24:12.916078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.35it/s, epoch=0, loss=5.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['भारत', 'के', 'बाद', 'के', 'बाद', 'के', 'बाद', 'के', 'बाद', ',', 'वह', 'भी', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वे', 'अपने', 'र', 'ब', 'की', 'तरह', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'है', 'कि', 'यह', 'है', 'कि', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['क्या', 'तुम', 'क्या', 'तुम', 'लोग', 'क्या', 'है', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.36it/s, epoch=1, loss=5.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['भारत', 'के', 'बाद', 'में', ',', 'भारत', 'के', 'बाद', 'में', ',', 'भारत', 'के', 'बाद', 'में', ',', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'ये', 'लोग', 'बहुत', 'से', 'पैदा', 'किया', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'भी', 'है', 'कि', 'यह', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['क्या', 'तुम', 'लोग', 'क्या', 'हो', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.38it/s, epoch=2, loss=4.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', ',', 'भारत', 'के', 'बाद', 'एक', 'और', 'स', 'ज', 'दा', 'में', 'एक', 'ही', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'लोगों', 'को', 'बहुत', 'अच्छा', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'एक', 'बहुत', 'बड़ी', 'बहुत', 'अच्छा', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['क्या', 'तुम', 'ने', 'तुम्', 'ह', 'ें', 'क्या', 'माल', 'ूम', 'कि', 'क्या', 'हो', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.36it/s, epoch=3, loss=4.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'भारत', 'के', 'बाद', 'एक', 'और', 'अन्य', 'देशों', 'में', ',', 'भारत', 'के', 'बाद', 'में', ',', 'भारत', 'की', 'तरफ', 'देखा', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['यह', 'बहुत', 'अच्छा', 'है', 'कि', 'इन', 'दिन', 'बहुत', 'बड़ा', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'पुस्तक', 'लगभग', 'पूरी', 'तरह', 'से', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['क्या', 'तुम', 'उस', 'दिन', 'क्या', 'माल', 'ूम', 'हो', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.34it/s, epoch=4, loss=3.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', ',', 'भारत', 'के', 'बाद', 'में', ',', 'भारत', 'के', 'बाद', ',', 'भारत', 'के', 'बाद', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'बहुत', 'से', 'बहुत', 'अच्छा', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'पुस्तक', 'है', 'कि', 'अब', 'भी', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['क्या', 'तुम', 'ने', 'देखा', 'उस', 'दिन', 'कब', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.38it/s, epoch=5, loss=3.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', ',', 'भारत', 'के', 'बाद', 'एक', 'और', 'ऊंच', 'े', 'को', 'भारत', 'में', 'देखा', 'गया', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'लोगों', 'को', 'बहुत', 'बड़े', 'बड़े', 'बड़े', 'बड़े', 'बड़े', 'हैं', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'शायद', 'बहुत', 'बड़ा', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम', 'उस', 'दिन', 'क्या', 'माल', 'ूम', 'कि', 'तुम', 'दिन', 'क्या', 'हो', 'गया', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.39it/s, epoch=6, loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'भारत', 'के', 'बाद', 'के', 'बाद', 'से', 'बाहर', 'निकल', 'गये', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'बहुत', 'सारे', 'लोगों', 'की', 'बहुत', 'बड़े', 'बड़े', 'बड़े', 'बड़े', 'बात', 'की', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'शायद', 'बहुत', 'कम', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम्', 'ह', 'ें', 'क्या', 'माल', 'ूम', 'हाव', 'िया', 'की', 'गई', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:54<00:00,  6.41it/s, epoch=7, loss=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'सी', 'प', 'र्व', 'के', 'बाद', 'एक', 'दूसरे', 'के', 'बाद', ',', 'भारत', 'के', 'बाद', 'भी', 'गया', '.', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'दिनों', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वास्तव', 'में', 'इतनी', 'सफलता', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम्', 'ह', 'ें', 'क्या', 'माल', 'ूम', 'कि', 'उस', 'दिन', 'क्या', 'है', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.37it/s, epoch=8, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'सी', 'एस', 'सी', 'टी', 'क', 'मे', 'टी', 'के', 'बाद', 'भारत', 'के', 'बाद', 'एक', 'और', 'द', 'याल', 'ु', 'गया', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'अच्छे', 'स्थान', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वास्तव', 'में', 'इतनी', 'सफलता', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तो', 'तुम', '(', 'ख़', 'ास', ')', 'बह', 'ूत', '(', 'ख़', 'ास', 'किस', 'किस', 'किस', 'चीज़', 'ने', 'किस', 'चीज़', 'ों', 'को', 'किस', 'चीज़', 'े', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.34it/s, epoch=9, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'तकनीकी', 'भारत', 'के', 'बाद', ',', 'भारत', 'को', 'घर', 'के', 'बाद', 'धीरे', '-', 'धीरे', 'बढ़ने', 'लगा', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'सब', 'े', 'राष्ट्र', 'के', 'सामने', 'यह', 'काम', 'कर', 'रही', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'निर्णय', 'बहुत', 'मुश्किल', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम्', 'ह', 'ें', 'क्या', 'माल', 'ूम', 'कि', 'को', 'क्या', 'हुआ', 'है', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.33it/s, epoch=10, loss=0.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'सी', 'एस', 'के', 'उपरांत', 'घ', 'ुलन', 'शील', ',', 'भारत', 'की', 'एक', '-', 'सम', 'र्थ', '-', 'सी', 'टी', 'के', 'बाद', 'बनी', 'ं', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'के', 'बाद', 'में', 'बहुत', 'अच्छा', 'होते', 'हैं', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'आप', 'क्या', 'जान', 'ना', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['और', 'तुम', 'तो', 'क्या', 'कह', 'रहे', 'हो', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.35it/s, epoch=11, loss=0.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'घ', 'बर', 'दार', 'के', 'बाद', 'एक', 'और', 'त', 'भारत', 'में', ',', 'भारत', 'में', 'रहते', 'हैं', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'सच', 'मु', 'च', 'सब', 'अच्छा', 'दिनों', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वि', 'लो', '-', 'चि', 'ह्न', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['(', 'आ', 'ख़', 'िर', ')', 'तुम्', 'ह', 'ें', 'क्या', 'हुआ', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:54<00:00,  6.40it/s, epoch=12, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'घ', 'म', 'के', 'बाद', 'भारत', 'ने', 'घ', 'ौ', 'मे', 'शु', 'न', 'के', 'बाद', 'में', 'अन्य', 'भागों', 'में', 'से', 'दिया', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'दिनों', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'निर्णय', 'बहुत', 'कठिन', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['(', 'आ', 'ख़', 'िर', ')', 'तुम', 'कह', 'दो', 'कि', 'तुम', 'को', 'क्या', 'कर', 'रहे', 'थे', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.37it/s, epoch=13, loss=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'घ', 'बर', 'दार', 'से', 'बाहर', 'आने', 'वाला', 'एक', 'और', 'त', 'भारत', 'के', 'बाद', 'अंक', 'गए', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'बहुत', 'बड़े', 'बड़े', 'दिनों', 'तक', 'ब्ब', 'त', 'रहे', 'है', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वि', 'ने', 'वाले', 'हैं', 'जो', 'ह', 'क़', '-', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम्', 'ह', 'ें', 'तो', 'कह', 'ां', 'श', 'भी', 'गया', 'था', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.37it/s, epoch=14, loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'इं', 'स', 'म्म', 'य', 'ड', ',', 'भारत', 'के', 'बाद', 'एक', 'और', 'भ', 'ेंट', 'हुई', '.', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'दिनों', 'में', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'निर्णय', 'डे', 'स्', 'सा', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम', 'कह', 'ोग', 'े', 'कि', 'अब', 'क्या', 'कर', 'रहे', 'है', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.39it/s, epoch=15, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'ट्रै', 'किंग', 'के', 'बाहर', 'एक', 'और', 'भारत', 'में', 'दूसरा', 'आ', 'गया', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'अच्छे', 'हैं', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'कप', 'अपनी', 'पहचान', 'कर', 'रही', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['आप', 'कह', 'ें', 'तो', 'कह', 'ेंगे', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.36it/s, epoch=16, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'सी', 'वन', 'के', 'उपरांत', 'बाहर', 'से', 'भारत', 'के', 'कुछ', 'अवसर', 'बना', 'ली', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'अच्छा', 'बना', 'रहे', 'थे', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वस्तुओं', 'अपने', 'आप', 'में', 'जानकारी', 'पता', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['और', 'तुम', 'तो', 'क्या', 'कह', 'ो', 'कि', 'ये', 'लोग', 'े', 'रे', 'गु', 'दा', 'ूर', 'हो', 'गए', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.31it/s, epoch=17, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'भारत', ',', 'भारत', 'रत्न', 'की', 'एक', 'दूसरी', 'खेल', 'में', ',', 'भारत', 'वर्ष', 'से', 'बाहर', 'निकल', 'गया', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'इन', 'दिनों', 'बहुत', 'दिनों', 'तक', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'हमारी', 'भारी', 'मन', 'ाने', 'वाले', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['कि', 'तुम', 'लोग', '(', 'दो', 'बार', 'की', ')', 'क्यों', 'नहीं', 'त', 'स्', 'दी', 'क़', 'करते', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.38it/s, epoch=18, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'इं', 'टी', 'जी', 'टल', 'भारत', 'के', 'बाद', 'में', 'वे', 'स्वतंत्र', 'बन', 'गये', '।', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'सब', 'कुछ', 'अच्छी', 'तरह', 'से', 'बहुत', 'ही', 'गतिविधियों', 'का', 'हो', 'गया', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वस्तुओं', 'कठिन', 'है', 'कि', 'व्यक्ति', 'स्वतंत्र', 'स', 'जग', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['और', 'तुम', 'को', 'क्या', 'माल', 'ूम', 'की', 'सक', '़', 'र', 'क्या', 'थे', '?', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:55<00:00,  6.37it/s, epoch=19, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving CheckPoint\n",
      "Example 1:\n",
      "another plant, the aluminium corporation of india, came into existence after the war.\n",
      "['एक', 'और', 'घ', 'ुट', 'ने', 'के', 'बाद', 'किसी', 'अन्य', 'के', 'बाद', 'एक', 'और', 'चलते', 'तै', 'त', 'रा', 'ते', 'हैं', '.', '']\n",
      "\n",
      "\n",
      "Example 2:\n",
      "He is doing very good these days\n",
      "['वह', 'ये', 'बहुत', 'अच्छे', 'स्थान', 'है', '।', '']\n",
      "\n",
      "\n",
      "Example 3:\n",
      "this guy is totally mad\n",
      "['यह', 'वितरण', 'की', 'तरह', 'सबसे', 'अधिक', 'जान', 'है', '']\n",
      "\n",
      "\n",
      "Example 4:\n",
      "what were you saying that day?\n",
      "['तुम', 'कह', 'ोग', 'े', 'की', 'उस', 'दिन', 'कह', 'ां', 'रही', 'हो', '']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses.append(train(train_loader, model, optimizer, scaler, scheduler, loss_fn, epoch))\n",
    "    \n",
    "    # save checkpoint\n",
    "    checkpoint = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer':optimizer.state_dict()\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    # check accuracy  on test set\n",
    "    infer(sentences, model, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e547a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T14:43:08.226061Z",
     "iopub.status.busy": "2023-05-12T14:43:08.225670Z",
     "iopub.status.idle": "2023-05-12T14:43:08.230521Z",
     "shell.execute_reply": "2023-05-12T14:43:08.229454Z"
    },
    "papermill": {
     "duration": 1.142638,
     "end_time": "2023-05-12T14:43:08.233027",
     "exception": false,
     "start_time": "2023-05-12T14:43:07.090389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to perform good translation, we can do the following - \n",
    "# get more data, first try with some shorter sentences (or to check if transformer works well, take small data, try\n",
    "# overfitting it)\n",
    "# perform beam search decoding\n",
    "# train it for longer time and schedule the learning rate as given in the paper\n",
    "# use in built nn.Transformer class "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1215.24457,
   "end_time": "2023-05-12T14:43:15.683122",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-12T14:23:00.438552",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
